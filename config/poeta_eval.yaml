# PoETa Evaluation Configuration
# Run with: python src/poeta_evaluator.py

# Model configuration
model_name: "meta-llama/Llama-3.1-8B-Instruct"

# Tasks to evaluate (use 'all' for full benchmark, or list specific tasks)
# Common tasks: assin_rte_greedy, assin_sts_greedy, enem_greedy, bluex_launch_version_
tasks: 
  - assin_rte_greedy
  - enem_greedy

# Few-shot configuration
num_fewshot: 2

# Limit examples per task (null for full evaluation, use small number for testing)
limit: 10  # Set to null for production runs

# Prompt mode
prompt_modes: "dynamic-random"

# Device
device: "cuda"
batch_size: 1

# Activation interventions (null for baseline, or dict of layer_X-neuron_Y: multiplier)
# Example:
# activation_multipliers:
#   layer_10-neuron_512: 0.5
#   layer_15-neuron_256: 2.0
activation_multipliers: null

# Whether to run comparison mode (evaluates both baseline and intervened)
compare_baseline: false

# Output configuration  
output_dir: "runs"

# Logging options
save_logs: true  # Save terminal output to log file in run directory

# Path to task descriptions (relative to PoETa repo)
description_dict_path: "description.json"

# Hydra config to prevent outputs/ folder creation
hydra:
  output_subdir: null
  run:
    dir: .
